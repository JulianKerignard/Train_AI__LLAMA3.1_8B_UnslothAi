# fast_training.py
# Version optimis√©e pour la vitesse sur RTX 3080

import os
import torch
import argparse
from datasets import load_from_disk
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from transformers import Trainer, DataCollatorForLanguageModeling
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel
from transformers import BitsAndBytesConfig
from datetime import datetime


# Analyse des arguments
def parse_args():
    parser = argparse.ArgumentParser(description="Entra√Ænement rapide")
    parser.add_argument("--continue_training", action="store_true", help="Continuer l'entra√Ænement")
    parser.add_argument("--start_idx", type=int, default=0, help="Index de d√©part")
    parser.add_argument("--num_examples", type=int, default=30000, help="Nombre d'exemples")
    parser.add_argument("--batch_size", type=int, default=4, help="Taille du batch")  # Augment√© √† 4
    parser.add_argument("--gradient_accumulation", type=int, default=4, help="Accumulation")  # R√©duit √† 4
    parser.add_argument("--max_length", type=int, default=500, help="Longueur max")  # R√©duit √† 384
    parser.add_argument("--lora_rank", type=int, default=8, help="Rang LoRA")
    parser.add_argument("--max_steps", type=int, default=300, help="√âtapes max")  # R√©duit √† 150
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="Taux d'apprentissage")  # Augment√© √† 5e-4
    parser.add_argument("--output_dir", type=str, default="optimized_lora_model", help="Dossier de sortie")
    parser.add_argument("--use_flash_attention", action="store_true", help="Utiliser Flash Attention")
    parser.add_argument("--disable_gradient_checkpointing", action="store_true",
                        help="D√©sactiver gradient checkpointing")
    return parser.parse_args()


# Fonction pour nettoyer la m√©moire
def clean_memory():
    gc.collect()
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    print("üßπ M√©moire nettoy√©e")


# Point d'entr√©e principal
def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)

    # Afficher la configuration
    print("\n‚ö° ENTRA√éNEMENT RAPIDE SUR RTX 3080 ‚ö°")
    print(f"Batch size: {args.batch_size}, Accumulation: {args.gradient_accumulation}")
    print(f"Max length: {args.max_length}, √âtapes: {args.max_steps}")
    print(
        f"Learning rate: {args.learning_rate}, Gradient checkpointing: {'D√©sactiv√©' if args.disable_gradient_checkpointing else 'Activ√©'}")

    # === √âTAPE 1: CHARGEMENT DES DONN√âES ===
    print("\nüîç √âTAPE 1: CHARGEMENT DES DONN√âES")
    try:
        tokenized_data_path = os.path.join(args.output_dir, "prepared_data",
                                           f"tokenized_data_{args.start_idx}_{args.start_idx + args.num_examples}")

        if os.path.exists(tokenized_data_path):
            print(f"Chargement des donn√©es depuis {tokenized_data_path}...")
            tokenized_dataset = load_from_disk(tokenized_data_path)
            print(f"Donn√©es charg√©es: {len(tokenized_dataset)} exemples")

            # V√©rification des donn√©es
            sample = tokenized_dataset[0]
            print(f"Longueur de s√©quence: {len(sample['input_ids'])}")

            # Si les s√©quences sont trop longues, les tronquer
            if len(sample['input_ids']) > args.max_length:
                print(f"Troncation des s√©quences √† {args.max_length} tokens...")

                # Cr√©er une copie tronqu√©e
                truncated_dataset = []
                for example in tokenized_dataset:
                    truncated_example = {
                        'input_ids': example['input_ids'][:args.max_length],
                        'attention_mask': example['attention_mask'][:args.max_length],
                        'labels': example['labels'][:args.max_length]
                    }
                    truncated_dataset.append(truncated_example)

                from datasets import Dataset
                tokenized_dataset = Dataset.from_list(truncated_dataset)
                print(f"Donn√©es tronqu√©es: {len(tokenized_dataset)} exemples")
        else:
            print(f"‚ùå Donn√©es non trouv√©es dans {tokenized_data_path}")
            print("Veuillez d'abord pr√©parer les donn√©es avec le script de pr√©paration")
            exit(1)

        clean_memory()
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
        exit(1)

    # === √âTAPE 2: CHARGEMENT DU MOD√àLE ===
    print("\nüîß √âTAPE 2: CHARGEMENT DU MOD√àLE OPTIMIS√â")
    try:
        # Charger le tokenizer
        print("Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("unsloth/llama-3-8b-bnb-4bit")
        tokenizer.pad_token = tokenizer.eos_token

        # Options avanc√©es pour la vitesse
        model_kwargs = {
            "device_map": "auto",
            "torch_dtype": torch.float16,
            "use_cache": not args.disable_gradient_checkpointing,
            # Activer le cache si gradient checkpointing est d√©sactiv√©
        }

        # Configuration de quantification optimis√©e
        print("Configuration de quantification optimis√©e...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        model_kwargs["quantization_config"] = bnb_config

        # Si Flash Attention est activ√©
        if args.use_flash_attention:
            print("Activation de Flash Attention...")
            # Tentative d'activation de Flash Attention si disponible
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
            except Exception as e:
                print(f"Flash Attention non disponible: {e}")

        # Charger le mod√®le
        if args.continue_training and os.path.exists(args.output_dir):
            try:
                print(f"Chargement du mod√®le pr√©c√©dent depuis {args.output_dir}...")
                base_model = AutoModelForCausalLM.from_pretrained(
                    "unsloth/llama-3-8b-bnb-4bit", **model_kwargs
                )
                model = PeftModel.from_pretrained(base_model, args.output_dir)
                print("Mod√®le pr√©c√©dent charg√© avec succ√®s")
            except Exception as e:
                print(f"Erreur lors du chargement du mod√®le pr√©c√©dent: {e}")
                args.continue_training = False

        if not args.continue_training:
            print("Chargement d'un nouveau mod√®le...")
            model = AutoModelForCausalLM.from_pretrained(
                "unsloth/llama-3-8b-bnb-4bit", **model_kwargs
            )
            print("Mod√®le charg√© avec succ√®s")

            # Pr√©parer pour LoRA rapide
            print("Pr√©paration pour LoRA optimis√©...")
            model = prepare_model_for_kbit_training(model)

            # Configuration LoRA
            peft_config = LoraConfig(
                r=args.lora_rank,
                lora_alpha=16,
                target_modules=[
                    "q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"
                ],
                bias="none",
                lora_dropout=0.0,  # Pas de dropout pour plus de vitesse
                task_type="CAUSAL_LM",
            )
            model = get_peft_model(model, peft_config)

        # Essayer d'activer torch.compile pour la vitesse
        try:
            print("Tentative d'activation de torch.compile pour acc√©l√©ration...")
            if hasattr(torch, 'compile'):
                # Seules les fonctions forward des modules sp√©cifiques seront compil√©es
                model.base_model.model.model.layers[0].self_attn.forward = torch.compile(
                    model.base_model.model.model.layers[0].self_attn.forward,
                    mode="reduce-overhead"
                )
                print("Premi√®re couche d'attention compil√©e avec torch.compile")

                # Compiler plus de couches si n√©cessaire (ATTENTION: peut augmenter le temps de compilation)
                # for layer in model.base_model.model.model.layers[:4]:  # Compiler les 4 premi√®res couches
                #     layer.self_attn.forward = torch.compile(layer.self_attn.forward, mode="reduce-overhead")
            else:
                print("torch.compile non disponible dans cette version")
        except Exception as e:
            print(f"Impossible d'utiliser torch.compile: {e}")

        # Afficher les param√®tres
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Param√®tres totaux: {total_params:,}")
        print(f"Param√®tres entra√Ænables: {trainable_params:,} ({trainable_params / total_params:.2%})")

        clean_memory()
    except Exception as e:
        print(f"‚ùå Erreur lors de la configuration du mod√®le: {e}")
        exit(1)

    # === √âTAPE 3: CONFIGURATION DE L'ENTRA√éNEMENT RAPIDE ===
    print("\n‚öôÔ∏è √âTAPE 3: CONFIGURATION DE L'ENTRA√éNEMENT RAPIDE")
    try:
        # Dossier pour les checkpoints
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_dir = os.path.join(args.output_dir, f"checkpoint_{timestamp}")

        # Configuration optimis√©e pour la vitesse
        print("Configuration optimis√©e pour la vitesse...")
        training_args = TrainingArguments(
            output_dir=checkpoint_dir,
            per_device_train_batch_size=args.batch_size,
            gradient_accumulation_steps=args.gradient_accumulation,
            learning_rate=args.learning_rate,  # Learning rate plus √©lev√© pour converger plus vite
            lr_scheduler_type="cosine",  # Scheduler cosine plus efficace
            warmup_ratio=0.03,  # Warmup bas√© sur ratio plut√¥t que steps
            max_steps=args.max_steps,
            logging_steps=5,  # Logs plus fr√©quents pour suivre la progression
            save_steps=50,
            save_total_limit=2,  # Garder moins de checkpoints pour √©conomiser l'espace
            fp16=True,
            gradient_checkpointing=not args.disable_gradient_checkpointing,
            optim="adamw_torch_fused",  # Optimiseur fusionn√© plus rapide
            weight_decay=0.01,
            max_grad_norm=1.0,
            report_to="none",
            dataloader_num_workers=2,  # Utiliser 2 workers pour le chargement des donn√©es
            dataloader_pin_memory=True,  # √âpingler la m√©moire pour des transferts plus rapides
            remove_unused_columns=False,
            disable_tqdm=False,
            seed=42
        )

        # Data collator optimis√©
        print("Configuration du data collator optimis√©...")
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )

        # Configuration du Trainer
        print("Configuration du Trainer optimis√©...")
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer
        )

        clean_memory()
    except Exception as e:
        print(f"‚ùå Erreur lors de la configuration de l'entra√Ænement: {e}")
        exit(1)

    # === √âTAPE 4: LANCEMENT DE L'ENTRA√éNEMENT RAPIDE ===
    print("\nüöÄ √âTAPE 4: LANCEMENT DE L'ENTRA√éNEMENT RAPIDE")
    try:
        # Afficher l'utilisation de la m√©moire
        if torch.cuda.is_available():
            gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1024 ** 3
            gpu_mem_reserved = torch.cuda.max_memory_reserved() / 1024 ** 3
            print(f"M√©moire GPU avant entra√Ænement: {gpu_mem_alloc:.2f} GB allou√©s, {gpu_mem_reserved:.2f} GB r√©serv√©s")

        # D√©marrer l'entra√Ænement avec timer
        import time
        start_time = time.time()
        print(f"D√©marrage de l'entra√Ænement √† {datetime.now().strftime('%H:%M:%S')}...")

        # Afficher la configuration finale
        print(
            f"Configuration finale: batch={args.batch_size}*{args.gradient_accumulation}={args.batch_size * args.gradient_accumulation}, lr={args.learning_rate}")

        # Lancer l'entra√Ænement
        trainer.train()

        # Calculer le temps √©coul√©
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        print(f"Entra√Ænement termin√© en {int(hours)}h {int(minutes)}m {int(seconds)}s")

        # Afficher l'utilisation finale de la m√©moire
        if torch.cuda.is_available():
            gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1024 ** 3
            gpu_mem_reserved = torch.cuda.max_memory_reserved() / 1024 ** 3
            print(f"Pic de m√©moire GPU: {gpu_mem_alloc:.2f} GB allou√©s, {gpu_mem_reserved:.2f} GB r√©serv√©s")

        # Sauvegarder le mod√®le final
        print("Sauvegarde du mod√®le final...")
        trainer.save_model(args.output_dir)
        tokenizer.save_pretrained(args.output_dir)
        print(f"Mod√®le et tokenizer sauvegard√©s dans {args.output_dir}")

        # Cr√©er un fichier d'information
        with open(os.path.join(args.output_dir, "training_info.txt"), "w") as f:
            f.write(f"Entra√Ænement rapide termin√© le {datetime.now()}\n")
            f.write(f"Dur√©e: {int(hours)}h {int(minutes)}m {int(seconds)}s\n")
            f.write(f"Exemples: {len(tokenized_dataset)} (de l'index {args.start_idx})\n")
            f.write(
                f"Configuration: batch={args.batch_size}*{args.gradient_accumulation}, lr={args.learning_rate}, steps={args.max_steps}\n")
    except Exception as e:
        print(f"‚ùå Erreur pendant l'entra√Ænement: {e}")
        # Tenter de sauvegarder quand m√™me
        try:
            print("Tentative de sauvegarde malgr√© l'erreur...")
            trainer.save_model(args.output_dir)
            tokenizer.save_pretrained(args.output_dir)
            print(f"Mod√®le sauvegard√© dans {args.output_dir}")
        except Exception as e2:
            print(f"‚ùå Impossible de sauvegarder: {e2}")

    print("\n‚úÖ ENTRA√éNEMENT RAPIDE TERMIN√â")
    print(f"Mod√®le disponible dans: {args.output_dir}")
    print(
        f"Pour continuer avec de nouvelles donn√©es: python {__file__} --continue_training --start_idx {args.start_idx + args.num_examples}")


if __name__ == "__main__":
    main()